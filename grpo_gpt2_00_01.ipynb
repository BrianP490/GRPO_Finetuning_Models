{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0abfdc",
   "metadata": {},
   "source": [
    "Using GRPO to finetune the chat gpt2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5088d5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d6412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.0.2\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.5.1\n",
      "pandas version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch, tiktoken, time, os\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.nn import Module # For type hinting\n",
    "from typing import Tuple # Import Tuple for type hinting\n",
    "from tiktoken import Encoding  # For type hinting\n",
    "from utils import GPTModel, load_weights_into_gpt, download_and_load_gpt2, download_dataset_wrapper, download_and_unzip_spam_data\n",
    "\n",
    "# pkgs = [\"numpy\", \n",
    "#         \"tiktoken\", \n",
    "#         \"torch\",\n",
    "#         # \"tensorflow\", # For OpenAI's pretrained weights\n",
    "#         \"pandas\"\n",
    "#        ]\n",
    "# for p in pkgs:\n",
    "#     print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f58fba",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7688181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(data_file_path=\"./sms_spam_collection/SMSSpamCollection.tsv\", sep=\"\\t\", header=None, column_names=[\"Label\", \"Text\"], train_frac=0.7, validation_frac=0.15, store_directory=\"./sms_spam_collection/data_splits\"):\n",
    "    \"\"\"This function prepares the train, test, and validation datasets from the original data.\n",
    "    Code inspired from: https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb\n",
    "    Args:\n",
    "        data_file_path (str): Path to the original dataset.\n",
    "        sep (str): The separator used in the dataset file.\n",
    "        column_names (list): A list of strings representing the column names used to read the dataset file using pandas.\n",
    "        train_frac (float): The percentage of the data used for training.\n",
    "        validation_frac (float): The percentage of the overall data used for the validation dataset.\n",
    "        store_directory (str): The parent directory path that will contain the 3 datasets (train, test, and validation) .\n",
    "\n",
    "    Returns:\n",
    "        store_directory (str): The parent directory path containing the 3 datasets.\n",
    "        \"\"\"\n",
    "    \n",
    "    print(f\"'prepare_datasets' function call: Using data_file_path='{data_file_path}' to find the original dataset.\\n Using store_directory='{store_directory}' for the train, test, and validation dataset parent directory\")\n",
    "\n",
    "    # Construct the full paths for the output files\n",
    "    train_csv_path = os.path.join(store_directory, \"train.csv\")\n",
    "    test_csv_path = os.path.join(store_directory, \"test.csv\")\n",
    "    validation_csv_path = os.path.join(store_directory, \"validation.csv\")\n",
    "\n",
    "    if os.path.exists(train_csv_path) and os.path.exists(test_csv_path) and os.path.exists(validation_csv_path) :\n",
    "        print(f\"Train, Test, and Validation datasets detected in '{store_directory}', skipping generation\")\n",
    "    else:\n",
    "        print(f\"Datasets not found in '{store_directory}' or incomplete. Generating datasets...\")\n",
    "\n",
    "        df = pd.read_csv(data_file_path, sep=sep, header=header, names=column_names)\n",
    "\n",
    "        # Count the instances of \"spam\"\n",
    "        num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "        \n",
    "        # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
    "        ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "        \n",
    "        # Combine ham \"subset\" with \"spam\"\n",
    "        balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "        balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "        # Shuffle the entire DataFrame\n",
    "        balanced_df = balanced_df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "        # Calculate split indices\n",
    "        train_end = int(len(balanced_df) * train_frac)\n",
    "        validation_end = train_end + int(len(balanced_df) * validation_frac)\n",
    "\n",
    "        # Split the DataFrame\n",
    "        train_df = balanced_df[:train_end]\n",
    "        test_df = balanced_df[validation_end:]\n",
    "        validation_df = balanced_df[train_end:validation_end]\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(store_directory, exist_ok=True) \n",
    "\n",
    "        train_df.to_csv(train_csv_path, index=None)\n",
    "        test_df.to_csv(test_csv_path, index=None)\n",
    "        validation_df.to_csv(validation_csv_path, index=None)\n",
    "\n",
    "    print(f\"'prepare_datasets' function returning: {store_directory} as parent directory.\")\n",
    "    \n",
    "    return store_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09fd3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    \"\"\"Dataset class to turn text into tokenized inputs\n",
    "    Original Code in: https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb\"\"\"\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_file)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found: {csv_file}\")\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]    # For each row in the text section of the pandas data frame tokenize the text string(sentence); creates list of token IDs for each example/item of the text data\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07846c20",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67934549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_datasets_and_dataloaders_pipeline(data_file_path=None, store_directory=None, batch_size=64, num_workers=0, pin_memory=False, drop_last=True) -> tuple[Dataset, Dataset, Dataset, DataLoader, DataLoader, DataLoader, Encoding]:\n",
    "    \"\"\"This pipeline does the following: calls function to prepare the train, test, and validation datasets, creates the dataloaders for each dataset, initializes the tokenizer, and returns them. Original code: https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb\n",
    "    Args:\n",
    "        data_file_path (str): Path to the original dataset.\n",
    "        sep (str): The separator used in the dataset file.\n",
    "        column_names (list): A list of strings representing the column names used to read the dataset file using pandas.\n",
    "        train_frac (float): The percentage of the data used for training.\n",
    "        validation_frac (float): The percentage of the overall data used for the validation dataset.\n",
    "        store_directory (str): The parent directory path that will contain the 3 datasets (train, test, and validation).\n",
    "        batch_size (int): The dataloader's batch_size.\n",
    "        num_workers (int): The dataloader's number of workers.\n",
    "        pin_memory (bool): The dataloader's pin memory option.\n",
    "        drop_last (bool): The dataloader's drop_last option.\n",
    "\n",
    "    Returns: \n",
    "        train_dataset (Dataset): Dataset Class for the training dataset.\n",
    "        test_dataset (Dataset): Dataset Class for the test dataset.\n",
    "        validation_dataset (Dataset): Dataset Class for the validation dataset.\n",
    "        train_dataloader (DataLoader): The train dataloader.\n",
    "        test_dataloader (DataLoader): The test dataloader.\n",
    "        validation_dataloader (DataLoader): The validation dataloader.\n",
    "        tokenizer (Encoding): The tokenizer used to convert the text data into tokens.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Construct the arguments for prepare_datasets conditionally\n",
    "    prepare_args = {}\n",
    "    if data_file_path is not None:\n",
    "        prepare_args['data_file_path'] = data_file_path\n",
    "    if store_directory is not None:\n",
    "        prepare_args['store_directory'] = store_directory\n",
    "\n",
    "    # Prepare the 3 datasets files and return their parent directory\n",
    "    store_directory = prepare_datasets(**prepare_args)\n",
    "\n",
    "    train_dataset = SpamDataset(csv_file=os.path.join(store_directory, \"train.csv\"), tokenizer=tokenizer)\n",
    "    test_dataset = SpamDataset(csv_file=os.path.join(store_directory, \"test.csv\"), tokenizer=tokenizer)\n",
    "    validation_dataset = SpamDataset(csv_file=os.path.join(store_directory, \"validation.csv\"), tokenizer=tokenizer)\n",
    "\n",
    "    print(f\"Created Train dataset with '{len(train_dataset)}' samples\")\n",
    "    print(f\"Created Test dataset with '{len(test_dataset)}' samples\")\n",
    "    print(f\"Created Validation dataset with '{len(validation_dataset)}' samples\")\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n",
    "\n",
    "    print(f\"Created Train Dataloader with '{len(train_dataloader)}' batches.\")\n",
    "    print(f\"Created Test Dataloader with '{len(test_dataloader)}' batches.\")\n",
    "    print(f\"Created Validation dataset with '{len(validation_dataloader)}' batches.\")\n",
    "    print(f\"Each Dataloader has a batch_size of {batch_size}\")\n",
    "    \n",
    "    return (train_dataset, test_dataset, validation_dataset, train_dataloader, test_dataloader, validation_dataloader, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebf8e7",
   "metadata": {},
   "source": [
    "## Building Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_new_policy(base_config, chosen_model=\"gpt2-small (124M)\", num_classes=2) -> GPTModel:\n",
    "    \"\"\"Build and load in the GPT2 model. Swap out the Head layer, and freeze up to the last Transformer module for transfer learning. Code Inspired from: https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb\n",
    "    Args:\n",
    "        base_config (dict): The base configurations of the gpt2 model indicating vocab_size, context_length, drop_rate, and qkv_bias.\n",
    "        chosen_model (str): The specific gpt2 model to construct.\n",
    "        num_classes (int): The amount of classes in the classification task.\n",
    "    Returns:\n",
    "        model (GPTModel): The constructed Transformer model for classification.\"\"\"\n",
    "    \n",
    "    model_configs = {\n",
    "        \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "        \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "        \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "        \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "\n",
    "    base_config.update(model_configs[chosen_model]) # Add the emb_dim, n_layers, and n_heads to the config\n",
    "\n",
    "    model_size = chosen_model.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")    # Extract the number of parameters from the chosen_model\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "    \n",
    "    print(f\"Downloading weights of chosen model and loading them to initialized instance\")\n",
    "    settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2_models_directory\")\n",
    "\n",
    "    model = GPTModel(base_config)\n",
    "\n",
    "    load_weights_into_gpt(model, params)\n",
    "    \n",
    "    for param in model.parameters(): # Freeze model parameters\n",
    "        param.requires_grad = False \n",
    "\n",
    "    # Unfreeze the last transformer block\n",
    "    for param in model.trf_blocks[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Unfreeze the final layer normalizing layer\n",
    "    for param in model.final_norm.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model.out_head = torch.nn.Linear(in_features=base_config[\"emb_dim\"], out_features=num_classes) # Reconfigure the output layer for the classification task\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a28e2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_old_policy(base_config, chosen_model=\"gpt2-small (124M)\", num_classes = 2) -> GPTModel:\n",
    "    \"\"\"Construct the GPT2 model architecture without loading the weights. Code inspired from: https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb\n",
    "    Args:\n",
    "        base_config (dict): The base configurations of the gpt2 model indicating vocab_size, context_length, drop_rate, and qkv_bias.\n",
    "        chosen_model (str): The specific gpt2 model to construct.\n",
    "        num_classes (int): The amount of classes in the classification task.\n",
    "    Returns:\n",
    "        model (GPTModel): The constructed Transformer model for classification.\"\"\"\n",
    "    \n",
    "    model_configs = {\n",
    "        \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "        \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "        \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "        \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "\n",
    "    base_config.update(model_configs[chosen_model]) # Add the emb_dim, n_layers, and n_heads to the config\n",
    "\n",
    "    model_size = chosen_model.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "    model = GPTModel(base_config)\n",
    "\n",
    "    model.out_head = torch.nn.Linear(in_features=base_config[\"emb_dim\"], out_features=num_classes) # Reconfigure the output layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ee497",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f5ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discounted_rewards(predictions, batch_labels, gamma) -> torch.tensor:\n",
    "    \"\"\"A general function to calculate the discounted rewards for each of the model's trajectories. For this implementation, however, use non-discounted rewards.\n",
    "    Args:\n",
    "        predictions (torch.tensor): A flattened 1-d tensor containing the policy's predictions.\n",
    "        batch_labels (torch.tensor): A flattened 1-d tensor containing the target predictions.\n",
    "        gamma (float): The amount of discount to apply to future rewards [Not Used in this Implementation].\n",
    "    Returns:\n",
    "        disc_rewards (torch.tensor): The 1-d tensor containing the discounted rewards of each of the model's trajectories.\n",
    "        \"\"\"\n",
    "    disc_rewards = (predictions == batch_labels).float()    # Simple comparison to evaluate rewards for each example; output a tensor of floats\n",
    "    return disc_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca4df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_epoch_stats(epoch, epoch_limit, total_loss, ratio, entropy) -> None:\n",
    "    print(f\"=====================  [Epoch ({epoch})]  =====================\")\n",
    "    print(\"Last k_epoch stats:\")\n",
    "    print(f\"Loss: {total_loss:.7f} | Ratio: {ratio:.7f} | Entropy Term: {entropy:.7f}\")\n",
    "    print(f\"===========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9edd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(Policy: Module, dataloader: DataLoader, current_epoch: int = None, max_epochs: int=None, device: str = 'cpu') -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the policy model (greedy version) on a given dataset.\n",
    "    Args:\n",
    "        Policy (Module): The Policy Model.\n",
    "        dataloader (DataLoader): The dataloader to evaluate with.\n",
    "        current_epoch (int): The current epoch [optional].\n",
    "        max_epochs (int): The maximum number of epochs [optional].\n",
    "        device (str): The device that the calculations will take place on.\n",
    "    Returns:\n",
    "        accuracy (float): The calculated accuracy.\n",
    "    \"\"\"\n",
    "    Policy.eval()   # Turn off dropout layers and prevent grad tracking\n",
    "\n",
    "    # Dataset check before continuing\n",
    "    if len(dataloader.dataset) == 0: # Check the underlying dataset size\n",
    "        print(f\"Warning: Evaluation dataset is empty. Skipping accuracy calculation.\")\n",
    "        return float('nan')\n",
    "    \n",
    "    accuracy, num_correct, num_of_samples = 0.0, 0.0, 0.0\n",
    "\n",
    "    Softmax_lyr = torch.nn.Softmax(dim=-1)  # The layer to transform the logits to probabilities\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in dataloader:\n",
    "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device) # Move the training data to the target device\n",
    "\n",
    "            logits = Policy(batch_inputs)[:,-1,:]   # Get logits from model and only focus on the last iterations of each sample!!!\n",
    "            # print(old_logits)\n",
    "            \n",
    "            classification_probabilities = Softmax_lyr(logits)\n",
    "            class_predictions = torch.argmax(classification_probabilities, dim=-1).flatten()\n",
    "            num_of_samples += batch_labels.size(0)\n",
    "            num_correct += sum((class_predictions == batch_labels).float()).item()\n",
    "    \n",
    "    accuracy = num_correct/num_of_samples\n",
    "    if current_epoch and max_epochs:   # If the function was called in the training loop\n",
    "        print(f\"===================  [Epoch ({current_epoch}/{max_epochs})]  ===================\")\n",
    "        print(f\"Entire Validation Dataset Accuracy: {accuracy:.4f}| {num_correct} / {num_of_samples} samples\")\n",
    "        print(f\"====================================================\")\n",
    "\n",
    "    else:   # If the function was called outside of the training loop\n",
    "        print(f\"===============================================\")\n",
    "        print(f\"Entire Dataset Accuracy: {accuracy:.4f} | {num_correct} / {num_of_samples} samples\")\n",
    "        print(f\"=====================================================\")\n",
    "\n",
    "            \n",
    "    Policy.train()  # Set back to training mode \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc421f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_spam_classify_single(Policy: GPTModel, input_text: str, tokenizer: Encoding, device='cpu'):\n",
    "    \"\"\"Used to test the Policy with a single messages to classify it as SPAM or NOT SPAM.\n",
    "    Args:\n",
    "        Policy (GPTModel): The Policy that will classify the text\n",
    "        input_text (list): The list containing the input texts.\n",
    "        tokenizer (Encoding): The tokenizer that turns text into tokens to feed the policy.\n",
    "        device (str): The device to run the Policy and input tokens to.\n",
    "    \"\"\"\n",
    "    Policy.eval().to(device)\n",
    "    Softmax_lyr = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    tokenized_text = tokenizer.encode(input_text)\n",
    "    torch_text=torch.tensor(tokenized_text).unsqueeze(0)    # turn into a tensor and add a batch dimension\n",
    "    model_inputs = torch_text.to(device)\n",
    "    # print(f\"torch_text: {torch_text} | {torch_text.shape}\")\n",
    "    # print(f\"model_inputs: {model_inputs} | {model_inputs.shape}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = Policy(model_inputs)[:,-1,:]\n",
    "        print(f\"logits: {logits}\")\n",
    "        Class_probabilities = Softmax_lyr(logits)\n",
    "    prediction = torch.argmax(input=Class_probabilities, dim=-1)\n",
    "    # print(f\"prediction: {prediction}\")\n",
    "\n",
    "    print(\"==================================================================\")\n",
    "    print(f\"Classifiying the following text as [SPAM or NOT SPAM]:\")\n",
    "    print(f\"'{input_text}'\")\n",
    "    print(f\"Prediction ... [ => {'SPAM' if prediction.item() == 1 else 'NOT SPAM'} <= ]\")\n",
    "    print(\"==================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b54b9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_spam_classify_batch(Policy: GPTModel, input_text: list, tokenizer: Encoding, device='cpu'):\n",
    "    \"\"\"Used to test the Policy with a batch of messages to classify them as SPAM or NOT SPAM.\n",
    "    Args:\n",
    "        Policy (GPTModel): The Policy that will classify the text\n",
    "        input_text (list): The list containing the input texts.\n",
    "        tokenizer (Encoding): The tokenizer that turns text into tokens to feed the policy.\n",
    "        device (str): The device to run the Policy and input tokens to.\n",
    "    \"\"\"\n",
    "    Policy.eval().to(device)\n",
    "    Softmax_lyr = torch.nn.Softmax(dim=-1)\n",
    "    pad_token_id=50256\n",
    "\n",
    "    tokenized_text = [\n",
    "            tokenizer.encode(text) for text in input_text    # For each row in the text section of the pandas data frame tokenize the text string(sentence); creates list of token IDs for each example/item of the text data\n",
    "        ]\n",
    "    \n",
    "    max_length = 0\n",
    "    for encoded_text in tokenized_text:\n",
    "        encoded_length = len(encoded_text)\n",
    "        if encoded_length > max_length:\n",
    "            max_length = encoded_length\n",
    "    \n",
    "    # Pad sequences to the longest sequence\n",
    "    encoded_texts = [\n",
    "        encoded_text + [pad_token_id] * (max_length - len(encoded_text))\n",
    "        for encoded_text in tokenized_text\n",
    "    ]\n",
    "    \n",
    "    torch_text=torch.tensor(encoded_texts, dtype=torch.long)    # Turn into a tensor\n",
    "    model_inputs = torch_text.to(device)\n",
    "    # print(f\"torch_text: {torch_text} | {torch_text.shape}\")\n",
    "    with torch.no_grad():\n",
    "        logits = Policy(model_inputs)[:,-1,:]\n",
    "        print(f\"logits: {logits}\")\n",
    "        Class_probabilities = Softmax_lyr(logits)\n",
    "    predictions = torch.argmax(input=Class_probabilities, dim=-1)\n",
    "    print(f\"predictions: {predictions}\")\n",
    "\n",
    "    bundle = zip(input_text, predictions)\n",
    "    print(f\"input_text:{input_text} | pred: {predictions}\")\n",
    "\n",
    "    for i, (text_str, pred) in enumerate(bundle):\n",
    "        print(\"==================================================================\")\n",
    "        print(f\"Classifiying the following text:\")\n",
    "        print(f\"[SPAM || NOT SPAM]: \\n'{text_str}'\")\n",
    "        print(f\"Prediction ... [ => {'SPAM' if pred == 1 else 'NOT SPAM'} <= ]\")\n",
    "        print(\"==================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4d3419",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75ae25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_train(model_config: dict, train_dataloader: DataLoader, validation_dataloader: DataLoader, gpt2_size=\"gpt2-small (124M)\", epochs=32, learning_rate=0.0003, batch_size=64, gamma=0.99, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5, entropy_coeff=0.05, log_iterations=10, eval_iterations=10, device=\"cpu\", num_envs:int=None) -> GPTModel:\n",
    "    \"\"\"The Group-Relative Policy Optimization Training function.\n",
    "\n",
    "    Args:\n",
    "        model_config (dict): The base configurations for building the policies.\n",
    "        train_dataloader (DataLoader): The dataloader for the training loop.\n",
    "        validation_dataloader (DataLoader): The dataloader for the validation loop.\n",
    "        gpt2_size (str): The GPT2 model and parameter choice (e.g. 'gpt2-small (124M)').\n",
    "        epochs (int): The number of times the outer loop is performed.\n",
    "        learning_rate (float): The hyperparameter that affects how much the model's parameters learn on each update iteration.\n",
    "        batch_size (int): The number of training examples that the Old Policy gathers to perform a GRPO update [Not Used in this Implementation].\n",
    "        gamma (float): The discount rate used to calculate discounted rewards.\n",
    "        k_epochs (int): The number of inner iterations used to update Policy New.\n",
    "        epsilon (float): The hyperparameter that affects the clipping of the 'R1_ratio'.\n",
    "        beta_kl (float): The hyperparameter that adjusts how much the KL Divergence term affects the overall GRPO Loss.\n",
    "        max_grad_norm (float): Used to promote numerical stability and prevent exploding gradients.\n",
    "        entropy_coeff (float): The hyperparameter that adjusts how much the entropy term affects the overall GRPO Loss.\n",
    "        log_iterations (int): Used to log information about the state of the New Policy.\n",
    "        eval_iterations (int): Used to run an evaluation of the New Policy.\n",
    "        device (str): The device that the model will be trained on.\n",
    "        num_envs (int): The number of parallel training environments that will be used during training [Not Used in this Implementation].\n",
    "\n",
    "    Returns: \n",
    "        Policy_New (GPTModel): The Trained Model in evaluation mode.\n",
    "    \"\"\"\n",
    "    # print(f\"Training Policy on {device} with {epochs} main epochs, {k_epochs} inner epochs, {learning_rate} learning rate, batch size={batch_size}, KL beta={beta_kl}, gamma={gamma}, epsilon={epsilon}, beta_kl={beta_kl}, max_grad_norm={max_grad_norm}, entropy_coeff={entropy_coeff}.\")\n",
    "    print(f\"Training Policy on {device} with {epochs} main epochs, {k_epochs} inner epochs, {learning_rate} learning rate, KL beta={beta_kl}, epsilon={epsilon}, beta_kl={beta_kl}, max_grad_norm={max_grad_norm}, entropy_coeff={entropy_coeff}.\")\n",
    "    print(f\"Using gpt2 size: '{gpt2_size}', logging every {log_iterations} epoch iterations, evaluating every {eval_iterations} epoch iterations.\")\n",
    "\n",
    "    Policy_New = build_new_policy(model_config, chosen_model=gpt2_size, num_classes=2).to(device)   # STEP 1 || \n",
    "    Policy_New.train()\n",
    "    # Policy_New = torch.compile(Policy_New) # To reap efficiency benefits ; not working due to Triton dependency\n",
    "\n",
    "    # STEPS 2 || For I iterations --> OMITTED\n",
    "    # STEPS 3 || Initialize a reference model --> OMITTED\n",
    "\n",
    "    optimizer = optim.Adam(params=Policy_New.parameters(), lr=learning_rate)\n",
    "\n",
    "    Policy_Old = build_old_policy(model_config, chosen_model=gpt2_size, num_classes=2).to(device)\n",
    "    Policy_Old.eval()\n",
    "    # Policy_Old = torch.compile(Policy_Old)\n",
    "\n",
    "    classifier_lyr = torch.nn.Softmax(dim=-1)   # For validation loop\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=f\">>>>>>>>>>>>>>>>>>>>>\\nMain Epoch (Outer Loop)\", leave=True):     # STEP 4 || \n",
    "        # STEP 5 || Sample a batch D_b from D\n",
    "        batch_inputs, batch_labels = next(iter(train_dataloader))\n",
    "        batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device) # move the training data to the target device\n",
    "        # print(f\"batch_inputs shape: {batch_inputs.shape}\")\n",
    "        # print(f\"batch_labels shape: {batch_labels.shape}\")\n",
    "\n",
    "        # STEP 6 || Update the old policy model PI old <- PI new\n",
    "        Policy_Old.load_state_dict(Policy_New.state_dict())\n",
    "        \n",
    "        # STEP 7 || Collect a Batch of Experiences Using the Old Policy\n",
    "        with torch.no_grad():\n",
    "            old_logits = Policy_Old(batch_inputs)[:,-1,:]   # Get logits from model and only focus on the last iterations of each sample\n",
    "            # print(old_logits)\n",
    "            old_dist = torch.distributions.Categorical(logits=old_logits) # Create a distribution to sample from\n",
    "            old_predictions = old_dist.sample() # Tensor of shape (batch_size,) ; list of predictions\n",
    "            # print(f\"old_predictions: \\n{old_predictions[:10]}\")\n",
    "            # print(f\"batch_labels True Values: \\n{batch_labels[:10]}\")\n",
    "            old_log_probs = old_dist.log_prob(old_predictions)\n",
    "\n",
    "        # STEP 8 || Calculate \"Discounted\" Rewards for completed trajectories\n",
    "        discounted_rewards = calculate_discounted_rewards(old_predictions, batch_labels, gamma)    # Output is a 1-d Tensor with \"discounted\" rewards per each sample in batch\n",
    "        # print(\"Calculated discounted returns\")\n",
    "        # STEP 9 || Calculate the Advantage for each Trajectory using normalization\n",
    "        all_advantages_tensor = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "\n",
    "        Policy_New.train()  # Prepare Policy NN for updates\n",
    "\n",
    "        # STEP 10 || GRPO Optimization ---\n",
    "        for k_epoch in tqdm(range(k_epochs), desc=f\"Epoch {epoch+1}/{epochs} (Inner K-Epochs)\", leave=True):\n",
    "            print(f\"===========================  [({k_epoch+1}/{k_epochs})]  ==========================\\n\")\n",
    "            optimizer.zero_grad()   # Flush out all the accumulated gradients for the weights of the model-under-training!!!\n",
    "\n",
    "            new_logits = Policy_New(batch_inputs)[:,-1,:]   # Get logits from model and only focus on the last iterations of each sample!!!\n",
    "            new_dist = torch.distributions.Categorical(logits=new_logits)\n",
    "            new_log_probs = new_dist.log_prob(old_predictions)  # Get the log probability of choosing the same action that the old policy took using the new distribution\n",
    "            entropy = new_dist.entropy().mean() # Calculate entropy for regularization\n",
    "            # print(f\"Entropy of this k_epoch: {entropy}\")\n",
    "            \n",
    "            R1_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "            unclipped_surrogate = R1_ratio * all_advantages_tensor\n",
    "            clipped_surrogate = torch.clamp(input=R1_ratio, min=1.0-epsilon, max=1.0+epsilon) * all_advantages_tensor\n",
    "            # print(f\"unclipped_surrogate: \\n{unclipped_surrogate[:10]}\\nclipped_surrogate: \\n{clipped_surrogate[:10]}\")\n",
    "            \n",
    "            policy_loss = -torch.min(unclipped_surrogate, clipped_surrogate).mean()\n",
    "\n",
    "            # Calculate KL divergence per sample, then take the mean over the batch\n",
    "            # Note: Reusing the calculated logits from STEP #7\n",
    "            kl_div_per_sample = torch.distributions.kl.kl_divergence(p=new_dist, q=old_dist)\n",
    "            kl_loss = kl_div_per_sample.mean() # Mean over the batch\n",
    "\n",
    "            # Total Loss for GRPO\n",
    "            total_loss = policy_loss + beta_kl * kl_loss - entropy_coeff * entropy\n",
    "            # print(f\"KL Divergence Average Loss: {kl_loss}\")\n",
    "\n",
    "            # STEP 11 || Policy Updates\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(Policy_New.parameters(), max_grad_norm)\n",
    "            optimizer.step()    # Update policy parameters using gradient ascent\n",
    "                \n",
    "\n",
    "        # --- Logging and Evaluation ---\n",
    "        if (epoch + 1) % log_iterations == 0:\n",
    "            log_epoch_stats(epoch=epoch+1, epoch_limit=epochs, total_loss=total_loss.item(), ratio=R1_ratio.mean().item(), entropy=entropy)\n",
    "\n",
    "        if (epoch + 1) % eval_iterations == 0:\n",
    "            accuracy = evaluate_policy(Policy_New, validation_dataloader, current_epoch=epoch+1, max_epochs=epochs, device=device)\n",
    "                \n",
    "\n",
    "    Policy_New.eval()   # Change to eval mode for evaluation after training is complete\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return Policy_New # Return the trained policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e37bf",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args) -> int:\n",
    "    print(\"SETTING UP FOR TRAINING\")\n",
    "    \n",
    "    if args.device:     # Check if the user specified to use a CPU or GPU for training\n",
    "        device = args.device\n",
    "    else:\n",
    "        if args.use_cuda:   # Check if the user wanted to use CUDA if available.\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    SAVE_LOCATION = \"./models/Spam-Classifier-GPT2-Model.pt\"   # Define the model path and name of the trained model weights\n",
    "\n",
    "    BASE_CONFIG = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": True         # Query-key-value bias\n",
    "    }\n",
    "\n",
    "    # --- Data Preparation Pipeline --- \n",
    "    train_dataset, test_dataset, validation_dataset, train_dataloader, test_dataloader, validation_dataloader, tokenizer = initialize_datasets_and_dataloaders_pipeline(data_file_path=\"./sms_spam_collection/SMSSpamCollection.tsv\", store_directory=\"./sms_spam_collection/data_splits\", num_workers=args.dataloader_num_workers, batch_size=args.dataloader_batch_size, pin_memory=args.dataloader_pin_memory)\n",
    "\n",
    "    print(\"BEGINNING TRAINING SCRIPT\")\n",
    "    start_time=time.time()\n",
    "\n",
    "    trained_policy = grpo_train(\n",
    "        model_config=BASE_CONFIG,\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=validation_dataloader,\n",
    "        gpt2_size=args.gpt2_size,\n",
    "        epochs=args.epochs,\n",
    "        learning_rate=args.learning_rate,\n",
    "        batch_size=args.batch_size, # Significantly larger batch size recommended for stability\n",
    "        gamma=args.gamma,\n",
    "        k_epochs=args.k_epochs,\n",
    "        epsilon=args.epsilon,\n",
    "        beta_kl=args.beta_kl,\n",
    "        entropy_coeff=args.entropy_coeff,\n",
    "        log_iterations=args.log_iterations,\n",
    "        eval_iterations=args.eval_iterations,\n",
    "        device=device,\n",
    "        num_envs=args.num_envs\n",
    "    )\n",
    "    end_time=time.time()\n",
    "\n",
    "    # --- Calculate Training Time --- \n",
    "\n",
    "    elapsed_time= end_time - start_time\n",
    "    hrs = int(elapsed_time / 3600)\n",
    "    min = int((elapsed_time % 3600) / 60)\n",
    "    seconds_remaining = elapsed_time - (hrs * 3600 ) - (min * 60)\n",
    "\n",
    "    print(f\"FINISHED MODEL TRAINING. \\nTRAINING TOOK: {hrs} Hours, {min} Minutes, and {seconds_remaining:.3f} Seconds\")\n",
    "\n",
    "    # --- Testing Trained Model --- \n",
    "    print(\"\\nTESTING THE TRAINED POLICY:\")\n",
    "\n",
    "    test_dataset_accuracy = evaluate_policy(trained_policy, test_dataloader, current_epoch=None, max_epochs=None, device=device)\n",
    "\n",
    "    # ---  Saving Model Section  ---   \n",
    "\n",
    "    if args.save_model:     # Check if the user wants to save the trained model weights\n",
    "        if args.model_output_path:     # Check if the user specified a target save location\n",
    "            SAVE_LOCATION=args.model_output_path\n",
    "\n",
    "        parent_dir = os.path.dirname(SAVE_LOCATION)\n",
    "\n",
    "        # If parent_dir is empty, it means the SAVE_LOCATION is just a filename\n",
    "        # in the current directory, so no new directories need to be created.\n",
    "        if parent_dir and parent_dir != '.':\n",
    "            try:\n",
    "                os.makedirs(parent_dir, exist_ok=True)\n",
    "                print(f\"Parent directory '{parent_dir}' created.\")\n",
    "            except OSError as e:\n",
    "                print(f\"Error creating directory {parent_dir}: {e}\")\n",
    "                # Depending on your application, you might want to exit or handle this more gracefully\n",
    "                # For example, fall back to a default save location or skip saving.\n",
    "        \n",
    "        try:\n",
    "            torch.save(trained_policy.state_dict(), f=SAVE_LOCATION)\n",
    "            print(f\"Model weights saved in: {SAVE_LOCATION}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model to {SAVE_LOCATION}: {e}\")\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab59e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming you have a way to call this function, e.g., in a main block)\n",
    "if __name__ == '__main__':\n",
    "    # --- Begin Timing Main Script Execution Time ---\n",
    "    start_time=time.time()\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Train and evaluate a GPT-based Spam Classifier using GRPO.\")\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=8,\n",
    "        help='(int, default=8) Number of training epochs to run.')\n",
    "\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0003,\n",
    "        help='(float, default=0.0003) Learning rate used by the optimizer.')\n",
    "\n",
    "    parser.add_argument('--dataloader_batch_size', type=int, default=64,\n",
    "        help='(int, default=64) Batch size used by the dataloaders for training, validation, and testing.')\n",
    "\n",
    "    parser.add_argument('--dataloader_pin_memory', action='store_false',\n",
    "        help='(bool, default=True) Disable pinned memory in dataloaders (enabled by default).')\n",
    "\n",
    "    parser.add_argument('--dataloader_num_workers', type=int, default=0,\n",
    "        help='(int, default=0) Number of subprocesses to use for data loading.')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=1024,\n",
    "        help='(int, default=1024) Batch size for collecting experience during training. [Not Used in this Implementation]')\n",
    "\n",
    "    parser.add_argument('--gpt2_size', type=str, default=\"gpt2-small (124M)\",\n",
    "        help='(str, default=\"gpt2-small (124M)\") GPT-2 model variant to use (e.g., \"gpt2-small (124M)\", \"gpt2-medium (355M)\", \"gpt2-large (774M)\", \"gpt2-xl (1558M)\").')\n",
    "\n",
    "    parser.add_argument('--k_epochs', type=int, default=32,\n",
    "        help='(int, default=32) Number of GRPO update steps per epoch.')\n",
    "\n",
    "    parser.add_argument('--epsilon', type=float, default=0.2,\n",
    "        help='(float, default=0.2) Clipping parameter for policy updates.')\n",
    "\n",
    "    parser.add_argument('--beta_kl', type=float, default=0.01,\n",
    "        help='(float, default=0.01) Coefficient for KL divergence penalty in GRPO loss calculation.')\n",
    "\n",
    "    parser.add_argument('--entropy_coeff', type=float, default=0.05,\n",
    "        help='(float, default=0.05) Coefficient for entropy regularization to encourage exploration.')\n",
    "\n",
    "    parser.add_argument('--log_iterations', type=int, default=2,\n",
    "        help='(int, default=2) Frequency (in iterations) to log training progress.')\n",
    "\n",
    "    parser.add_argument('--eval_iterations', type=int, default=2,\n",
    "        help='(int, default=2) Frequency (in iterations) to evaluate the model.')\n",
    "\n",
    "    parser.add_argument('--gamma', type=float, default=0.99,\n",
    "        help='(float, default=0.99) Discount factor for future rewards.')\n",
    "\n",
    "    parser.add_argument('--num_envs', type=int, default=16,\n",
    "        help='(int, default=16) Number of parallel environments used for experience collection. [Not Used in this Implementation]')\n",
    "\n",
    "    parser.add_argument('--use_cuda', action='store_true',\n",
    "        help='(bool, default=False) Enable CUDA for training if available.')\n",
    "\n",
    "    parser.add_argument('--device', type=str, default='cpu',\n",
    "        help='(str, default=\"cpu\") Device to use for training (e.g., \"cpu\", \"cuda:0\"). Overrides --use_cuda.')\n",
    "\n",
    "    parser.add_argument('--save_model', action='store_true',\n",
    "        help='(bool, default=False) Save the trained model after training.')\n",
    "\n",
    "    parser.add_argument('--model_output_path', type=str, default='models/Spam-Classifier-GPT2-Model.pt',\n",
    "        help='(str, default=\"models/Spam-Classifier-GPT2-Model.pt\") File path to save the trained model.')\n",
    "\n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    ret = main(args)\n",
    "\n",
    "    end_time=time.time()\n",
    "\n",
    "    # --- Calculate Main Script Execution Time --- \n",
    "\n",
    "    elapsed_time= end_time - start_time\n",
    "    hrs = int(elapsed_time / 3600)\n",
    "    min = int((elapsed_time % 3600) / 60)\n",
    "    seconds_remaining = elapsed_time - (hrs * 3600 ) - (min * 60)\n",
    "\n",
    "    print(f\"FINISHED MAIN SCRIPT\\nOVERALL DURATION: {hrs} Hours, {min} Minutes, and {seconds_remaining:.3f} Seconds\")\n",
    "    if ret == 0:\n",
    "        print(\"TERMINATING PROGRAM\")\n",
    "    else: \n",
    "        print(\"Main Scipt Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c6981",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e969d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretend that the argument parser will pass these arguments to the main function\n",
    "args = {\n",
    "    \"epochs\":8,\n",
    "    \"learning_rate\":0.0003,\n",
    "    \"dataloader_batch_size\":64,\n",
    "    \"dataloader_pin_memory\": True,  \n",
    "    \"dataloader_num_workers\": 0,    # Problem if I change this; slow for windows; try to modify within .py script\n",
    "    \"batch_size\":None, # Not needed in this build/project\n",
    "    \"gpt2_size\":'gpt2-small (124M)',\n",
    "    \"k_epochs\":64,       # GRPO Inner-loop\n",
    "    \"epsilon\":0.2,\n",
    "    \"beta_kl\":0.01,\n",
    "    \"entropy_coeff\":0.05,   # \n",
    "    \"log_iterations\":1,     # Log GRPO stats\n",
    "    \"eval_iterations\":1,    # Run model through evaluation at every \"x\" epochs\n",
    "    \"gamma\":None,   # Discounted Rewards\n",
    "    \"num_envs\":None,        # Not needed in this build/project\n",
    "    \"use_cuda\": None,\n",
    "    \"device\":'cpu',\n",
    "    \"save_model\":True,\n",
    "    \"model_output_path\":'models/Spam-Classifier-GPT2-Model.pt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc450d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, validation_dataset, train_dataloader, test_dataloader, validation_dataloader, tokenizer = initialize_datasets_and_dataloaders_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b46415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function call\n",
    "trained_policy = grpo_train(\n",
    "        model_config=BASE_CONFIG,\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=validation_dataloader,\n",
    "        gpt2_size=args[\"gpt2_size\"],\n",
    "        epochs=args[\"epochs\"],\n",
    "        learning_rate=args[\"learning_rate\"],\n",
    "        batch_size=args[\"batch_size\"], # Significantly larger batch size recommended for stability\n",
    "        k_epochs=args[\"k_epochs\"],\n",
    "        epsilon=args[\"epsilon\"],\n",
    "        beta_kl=args[\"beta_kl\"],\n",
    "        entropy_coeff=args[\"entropy_coeff\"],\n",
    "        log_iterations=args[\"log_iterations\"],\n",
    "        eval_iterations=args[\"eval_iterations\"],\n",
    "        gamma=args[\"gamma\"],\n",
    "        device=args[\"device\"],\n",
    "        num_envs=args[\"num_envs\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48722255",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text1 = \"Hey, wanna go out to watch the new fantastic four movie?\"\n",
    "input_text2 = \"XMAS Prize draws! We are trying to contact U. Todays draw shows that you have won a £2000 prize GUARANTEED. Call 09058094565 from land line. Valid 12hrs only\"\n",
    "input_text3= \"Had your contract mobile 11 Mnths? Latest Motorola, Nokia etc. all FREE! Double Mins & Text on Orange tariffs. TEXT YES for callback, no to remove from records\"\n",
    "input_text4 = \"I can't right this second, gotta hit people up first\"\n",
    "input_text5 = \"Sorry . I will be able to get to you. See you in the morning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93614c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = [input_text1, input_text2, input_text3, input_text4, input_text5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_spam_classify_batch(Policy=trained_policy, input_text=text_batch, tokenizer=tokenizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54916342",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_spam_classify_single(Policy=trained_policy, input_text=input_text1, tokenizer=tokenizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_spam_classify_single(Policy=trained_policy, input_text=input_text2, tokenizer=tokenizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57540fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_spam_classify_single(Policy=trained_policy, input_text=input_text3, tokenizer=tokenizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_spam_classify_single(Policy=trained_policy, input_text=input_text4, tokenizer=tokenizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db10363",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_spam_classify_single(Policy=trained_policy, input_text=input_text5, tokenizer=tokenizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815babda",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc= evaluate_policy(Policy=trained_policy, dataloader=train_dataloader, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
