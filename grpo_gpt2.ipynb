{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0abfdc",
   "metadata": {},
   "source": [
    "Using GRPO to finetune the chat gpt2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5088d5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773d6412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.23.5\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.5.1\n",
      "pandas version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch, tiktoken, time, os, tensorflow\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "pkgs = [\"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        # \"tensorflow\", # For OpenAI's pretrained weights\n",
    "        \"pandas\"\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c3b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.8\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f91b6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44482aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.previous_chapters import generate_text_simple, text_to_token_ids, token_ids_to_text,GPTModel, create_dataloader_v1, load_weights_into_gpt\n",
    "# Relative import from the gpt_download.py contained in this folder\n",
    "from utils.gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f58fba",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7688181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(data_file_path, sep=\"\\t\", header=None, column_names=[\"Label\", \"Text\"], train_frac=0.7, validation_frac=0.15, store_directory=\"./\"):\n",
    "    df = pd.read_csv(data_file_path, sep=sep, header=header, names=column_names)\n",
    "\n",
    "    # Count the instances of \"spam\"\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # Combine ham \"subset\" with \"spam\"\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "    # Shuffle the entire DataFrame\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(len(balanced_df) * train_frac)\n",
    "    validation_end = train_end + int(len(balanced_df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_df = balanced_df[:train_end]\n",
    "    validation_df = balanced_df[train_end:validation_end]\n",
    "    test_df = balanced_df[validation_end:]\n",
    "\n",
    "    train_df.to_csv(store_directory+\"/train.csv\", index=None)\n",
    "    validation_df.to_csv(store_directory+\"/validation.csv\", index=None)\n",
    "    test_df.to_csv(store_directory+\"/test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09fd3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_file)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found: {csv_file}\")\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]    # For each row in the text section of the pandas data frame tokenize the text string(sentence); creates list of token IDs for each example/item of the text data\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebf8e7",
   "metadata": {},
   "source": [
    "## Building Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923f77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_new_policy(base_config, chosen_model=\"gpt2-small (124M)\", num_classes = 2) -> GPTModel:\n",
    "    \"\"\"Build and load in the GPT2 model. Swap out the Head layer, and freeze up to the last Transformer module for transfer learning.\"\"\"\n",
    "    model_configs = {\n",
    "        \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "        \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "        \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "        \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "\n",
    "    base_config.update(model_configs[chosen_model]) # add the emb_dim, n_layers, and n_heads to the config\n",
    "\n",
    "    model_size = chosen_model.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "    settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "    model = GPTModel(base_config)\n",
    "\n",
    "    load_weights_into_gpt(model, params)\n",
    "    \n",
    "    for param in model.parameters(): # freeze model parameters\n",
    "        param.requires_grad = False \n",
    "\n",
    "    # Unfreeze the last transformer block\n",
    "    for param in model.trf_blocks[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Unfreeze the final layer normalizing layer\n",
    "    for param in model.final_norm.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model.out_head = torch.nn.Linear(in_features=base_config[\"emb_dim\"], out_features=num_classes) # reconfigure the output layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a28e2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_old_policy(base_config, chosen_model=\"gpt2-small (124M)\", num_classes = 2) -> GPTModel:\n",
    "    \"\"\"Construct just the model without loading the weights\"\"\"\n",
    "    model_configs = {\n",
    "        \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "        \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "        \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "        \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "\n",
    "    base_config.update(model_configs[chosen_model]) # add the emb_dim, n_layers, and n_heads to the config\n",
    "\n",
    "    model_size = chosen_model.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "    model = GPTModel(base_config)\n",
    "\n",
    "    model.out_head = torch.nn.Linear(in_features=base_config[\"emb_dim\"], out_features=num_classes) # reconfigure the output layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ee497",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f5ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discounted_rewards(predictions, batch_labels) -> torch.tensor:\n",
    "    \"\"\"For this implementation, use non-discounted rewards\"\"\"\n",
    "    disc_rewards = (predictions == batch_labels).float()    # Simple comparison to evaluate rewards for each example; output a tensor of floats\n",
    "    return disc_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4d3419",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75ae25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_train(model_config, train_dataloader, validation_dataloader, gpt_size=\"gpt2-small (124M)\", epochs=50, learning_rate=0.0001, batch_size=64, gamma=0.99, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5, entropy_coeff=0.01, log_iterations=10, eval_iterations=10, device=\"cpu\", num_envs=None):\n",
    "    print(f\"Training Policy on {device} with {epochs} main epochs, {k_epochs} inner epochs, {learning_rate} learning rate, batch size {batch_size}, KL beta {beta_kl}.\")\n",
    "    print(f\"Using gpt2 size:{gpt_size} , logging every {log_iterations} iterations, evaluating every {eval_iterations} iterations.\")\n",
    "\n",
    "\n",
    "    Policy_New = build_new_policy(model_config, chosen_model=gpt_size, num_classes=2).to(device)   # STEP 3 || \n",
    "    Policy_New.train()\n",
    "    # Policy_New = torch.compile(Policy_New) # to reap efficiency benefits \n",
    "\n",
    "    optimizer = optim.Adam(params=Policy_New.parameters(), lr=learning_rate)\n",
    "\n",
    "    Policy_Old = build_old_policy(model_config, chosen_model=gpt_size, num_classes=2).to(device)\n",
    "    Policy_Old.eval()\n",
    "    # Policy_Old = torch.compile(Policy_Old)\n",
    "\n",
    "    classifier_lyr = torch.nn.Softmax(dim=-1)   # For validation loop\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=f\"Main Epoch (Outer Loop)\", leave=False):     # STEP 4 || \n",
    "        # STEP 5 || Sample a batch D_b from D --> OMITTED \n",
    "        # STEP 6 || Update the old policy model PI old <- PI new\n",
    "        Policy_Old.load_state_dict(Policy_New.state_dict())\n",
    "        # print(\"loaded Policy Old Weights\")\n",
    "        # --- STEP 7 Collect a Batch of Experiences Using the Old Policy---\n",
    "        # for batch_inputs, batch_labels in train_dataloader:\n",
    "        #     pass\n",
    "        batch_inputs, batch_labels = next(iter(train_dataloader))\n",
    "        \n",
    "        batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device) # move the training data to the target device\n",
    "        print(\"Transferred Data\")\n",
    "        with torch.no_grad():\n",
    "            old_logits = Policy_Old(batch_inputs)[:,-1,:]   # Get logits from model and only focus on the last iterations of each sample\n",
    "            # print(old_logits)\n",
    "            old_dist = torch.distributions.Categorical(logits=old_logits) # Create a distribution to sample from\n",
    "            old_predictions = old_dist.sample() # Tensor of shape (batch_size,) ; list of predictions\n",
    "            # print(f\"old_predictions: {old_predictions}\")\n",
    "            old_log_probs = old_dist.log_prob(old_predictions)\n",
    "\n",
    "        # STEP 8 || Calculate \"Discounted\" Rewards for completed trajectories\n",
    "        discounted_rewards = calculate_discounted_rewards(old_predictions, batch_labels)    # Tensor with \"discounted\" rewards per each sample in batch\n",
    "        # print(\"Calculated discounted returns\")\n",
    "        # STEP 9 || Calculate the Advantage for each Trajectory using normalization\n",
    "        all_advantages_tensor = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "\n",
    "        Policy_New.train()  # Prepare NN for updates\n",
    "\n",
    "        # --- STEP 10 || GRPO Optimization ---\n",
    "        for k_epoch in tqdm(range(k_epochs), desc=f\"Epoch {epoch+1}/{epochs} (Inner K-Epochs)\", leave=True):\n",
    "            # print(\"Entered GRPO Optimization loop\")\n",
    "            optimizer.zero_grad()   # Flush out all the accumulated gradients for the weights of the model-under-training\n",
    "\n",
    "            new_logits = Policy_New(batch_inputs)[:,-1,:]   # Get logits from model and only focus on the last iterations of each sample\n",
    "            new_dist = torch.distributions.Categorical(logits=new_logits)\n",
    "            new_log_probs = new_dist.log_prob(old_predictions)  # Get the log probability of choosing the same action that the old policy took using the new distribution\n",
    "            entropy = new_dist.entropy().mean() # Calculate entropy for regularization\n",
    "\n",
    "            R1_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "            unclipped_surrogate = R1_ratio * all_advantages_tensor\n",
    "            clipped_surrogate = torch.clamp(input=R1_ratio, min=1.0-epsilon, max=1.0+epsilon) * all_advantages_tensor\n",
    "\n",
    "            policy_loss = -torch.min(unclipped_surrogate, clipped_surrogate).mean()\n",
    "\n",
    "            # --- KL Divergence Calculation ---\n",
    "            # Create distributions for old policies using the trajectory states\n",
    "            # with torch.no_grad():\n",
    "            #     old_logits = Policy_Old(all_states_tensor)\n",
    "            # old_dist = torch.distributions.Categorical(logits=old_logits)\n",
    "            # INSTEAD, just reusing the calculated logits from STEP #7\n",
    "\n",
    "            # Calculate KL divergence per sample, then take the mean over the batch\n",
    "            kl_div_per_sample = torch.distributions.kl.kl_divergence(p=new_dist, q=old_dist)\n",
    "            kl_loss = kl_div_per_sample.mean() # Mean over the batch\n",
    "\n",
    "            # Total Loss for GRPO\n",
    "            total_loss = policy_loss + beta_kl * kl_loss - entropy_coeff * entropy\n",
    "\n",
    "            # STEP 11 || Policy Updates\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(Policy_New.parameters(), max_grad_norm)\n",
    "            optimizer.step()    # Update policy parameters using gradient ascent\n",
    "                \n",
    "\n",
    "        # --- 4. Logging and Evaluation ---\n",
    "        if (epoch + 1) % log_iterations == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}, Ratio: {R1_ratio.mean().item():.5f}, Entropy Term: {entropy:.5f}\")\n",
    "\n",
    "        # if (epoch + 1) % eval_iterations == 0:\n",
    "            Policy_New.eval()   # Turn off dropout layers and prevent grad tracking\n",
    "            accuracy = 0.0\n",
    "            num_correct = 0.0\n",
    "            num_of_samples = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch_inputs, batch_labels in validation_dataloader:\n",
    "                    batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device) # move the training data to the target device\n",
    "                    logits = Policy_New(batch_inputs)[:,-1,:]\n",
    "                    classifications = classifier_lyr(logits)\n",
    "                    class_predictions = torch.argmax(classifications, dim=-1).flatten()\n",
    "                    num_of_samples += batch_labels.size(0)\n",
    "                    num_correct += sum((class_predictions == batch_labels).float()).item()\n",
    "                accuracy = num_correct/num_of_samples\n",
    "                print(f\"Epoch {epoch+1} | Entire Validation Dataset Accuracy: {accuracy:.4f}\")\n",
    "                \n",
    "\n",
    "    Policy_New.eval()   # Change to eval mode for evaluation after training is complete\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return Policy_New # Return the trained policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e37bf",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d74a4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretend that the argument parser will pass these arguments to the main function\n",
    "args = {\n",
    "    \"epochs\":256,\n",
    "    \"learning_rate\":0.0003,\n",
    "    \"dataloader_batch_size\":64,\n",
    "    \"dataloader_pin_memory\": True,  \n",
    "    \"dataloader_num_workers\": 0,    # Problem if I change this; slow for windows; try to modify within .py script\n",
    "    \"batch_size\":None, # Significantly larger batch size recommended for stability\n",
    "    \"gpt_size\":'gpt2-small (124M)',\n",
    "    \"k_epochs\":64,       # GRPO Inner-loop\n",
    "    \"epsilon\":0.2,\n",
    "    \"beta_kl\":0.01,\n",
    "    \"entropy_coeff\":0.001,\n",
    "    \"log_iterations\":64,\n",
    "    \"gamma\":None,   # Discounted Rewards\n",
    "    \"device\":'cuda',\n",
    "    \"num_envs\":None,\n",
    "    \"save_model\":True,\n",
    "    \"model_output_path\":'models/first.pt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f07896fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_datasets(data_file_path=\"./sms_spam_collection/SMSSpamCollection.tsv\", store_directory=\"./sms_spam_collection/data_splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aee6c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2ac70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/train.csv\", tokenizer=tokenizer)\n",
    "# test_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/test.csv\", tokenizer=tokenizer)\n",
    "validation_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/validation.csv\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2867b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=args[\"dataloader_batch_size\"], num_workers=args[\"dataloader_num_workers\"], pin_memory=args[\"dataloader_pin_memory\"], drop_last=True)\n",
    "# test_dataloader = DataLoader(dataset=test_dataset, batch_size=args[\"dataloader_batch_size\"], num_workers=args[\"dataloader_num_workers\"], pin_memory=args[\"dataloader_pin_memory\"], drop_last=True)\n",
    "validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=args[\"dataloader_batch_size\"], num_workers=args[\"dataloader_num_workers\"], pin_memory=args[\"dataloader_pin_memory\"], drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cabd13e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(validation_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee442850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "for batch_inputs, batch_labels in validation_dataloader:\n",
    "    print(f\"{batch_labels.size(0)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b65f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Policy on cuda with 1024 main epochs, 64 inner epochs, 0.0003 learning rate, batch size 1024, KL beta 0.01.\n",
      "Using gpt2 size:gpt2-small (124M) , logging every 256 iterations, evaluating every 10 iterations.\n",
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):   0%|          | 0/1024 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 13.01it/s]\n",
      "Main Epoch (Outer Loop):   0%|          | 1/1024 [00:04<1:24:25,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.98it/s]\n",
      "Main Epoch (Outer Loop):   0%|          | 2/1024 [00:09<1:25:07,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.97it/s]\n",
      "Main Epoch (Outer Loop):   0%|          | 3/1024 [00:15<1:25:17,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.95it/s]\n",
      "Main Epoch (Outer Loop):   0%|          | 4/1024 [00:20<1:25:23,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.95it/s]\n",
      "Main Epoch (Outer Loop):   0%|          | 5/1024 [00:25<1:25:22,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.91it/s]\n",
      "Main Epoch (Outer Loop):   1%|          | 6/1024 [00:30<1:25:24,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.85it/s]\n",
      "Main Epoch (Outer Loop):   1%|          | 7/1024 [00:35<1:25:31,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.83it/s]\n",
      "Main Epoch (Outer Loop):   1%|          | 8/1024 [00:40<1:25:36,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.81it/s]\n",
      "Main Epoch (Outer Loop):   1%|          | 9/1024 [00:45<1:25:40,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.92it/s]\n",
      "Main Epoch (Outer Loop):   1%|          | 10/1024 [00:50<1:25:28,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.92it/s]\n",
      "Main Epoch (Outer Loop):   1%|          | 11/1024 [00:55<1:25:18,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.93it/s]\n",
      "Main Epoch (Outer Loop):   1%|          | 12/1024 [01:00<1:25:11,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.92it/s]\n",
      "Main Epoch (Outer Loop):   1%|▏         | 13/1024 [01:05<1:25:05,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.94it/s]\n",
      "Main Epoch (Outer Loop):   1%|▏         | 14/1024 [01:10<1:24:57,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.94it/s]\n",
      "Main Epoch (Outer Loop):   1%|▏         | 15/1024 [01:15<1:24:52,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.88it/s]\n",
      "Main Epoch (Outer Loop):   2%|▏         | 16/1024 [01:20<1:24:54,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.93it/s]\n",
      "Main Epoch (Outer Loop):   2%|▏         | 17/1024 [01:25<1:24:43,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.92it/s]\n",
      "Main Epoch (Outer Loop):   2%|▏         | 18/1024 [01:30<1:24:38,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.81it/s]\n",
      "Main Epoch (Outer Loop):   2%|▏         | 19/1024 [01:35<1:24:42,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.88it/s]\n",
      "Main Epoch (Outer Loop):   2%|▏         | 20/1024 [01:40<1:24:35,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.93it/s]\n",
      "Main Epoch (Outer Loop):   2%|▏         | 21/1024 [01:45<1:24:30,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.89it/s]\n",
      "Main Epoch (Outer Loop):   2%|▏         | 22/1024 [01:51<1:24:26,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.90it/s]\n",
      "Main Epoch (Outer Loop):   2%|▏         | 23/1024 [01:56<1:24:23,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.94it/s]\n",
      "Main Epoch (Outer Loop):   2%|▏         | 24/1024 [02:01<1:24:16,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.87it/s]\n",
      "Main Epoch (Outer Loop):   2%|▏         | 25/1024 [02:06<1:24:16,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.79it/s]\n",
      "Main Epoch (Outer Loop):   3%|▎         | 26/1024 [02:11<1:24:20,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.88it/s]\n",
      "Main Epoch (Outer Loop):   3%|▎         | 27/1024 [02:16<1:24:11,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.66it/s]\n",
      "Main Epoch (Outer Loop):   3%|▎         | 28/1024 [02:21<1:24:31,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.88it/s]\n",
      "Main Epoch (Outer Loop):   3%|▎         | 29/1024 [02:26<1:24:18,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.89it/s]\n",
      "Main Epoch (Outer Loop):   3%|▎         | 30/1024 [02:31<1:24:07,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.90it/s]\n",
      "Main Epoch (Outer Loop):   3%|▎         | 31/1024 [02:36<1:23:57,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.89it/s]\n",
      "Main Epoch (Outer Loop):   3%|▎         | 32/1024 [02:41<1:23:50,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.84it/s]\n",
      "Main Epoch (Outer Loop):   3%|▎         | 33/1024 [02:46<1:23:48,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.84it/s]\n",
      "Main Epoch (Outer Loop):   3%|▎         | 34/1024 [02:51<1:23:51,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.86it/s]\n",
      "Main Epoch (Outer Loop):   3%|▎         | 35/1024 [02:57<1:23:41,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.84it/s]\n",
      "Main Epoch (Outer Loop):   4%|▎         | 36/1024 [03:02<1:23:38,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.84it/s]\n",
      "Main Epoch (Outer Loop):   4%|▎         | 37/1024 [03:07<1:23:34,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.86it/s]\n",
      "Main Epoch (Outer Loop):   4%|▎         | 38/1024 [03:12<1:23:25,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.84it/s]\n",
      "Main Epoch (Outer Loop):   4%|▍         | 39/1024 [03:17<1:23:23,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.89it/s]\n",
      "Main Epoch (Outer Loop):   4%|▍         | 40/1024 [03:22<1:23:12,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.87it/s]\n",
      "Main Epoch (Outer Loop):   4%|▍         | 41/1024 [03:27<1:23:06,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.90it/s]\n",
      "Main Epoch (Outer Loop):   4%|▍         | 42/1024 [03:32<1:22:58,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.86it/s]\n",
      "Main Epoch (Outer Loop):   4%|▍         | 43/1024 [03:37<1:22:57,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.62it/s]\n",
      "Main Epoch (Outer Loop):   4%|▍         | 44/1024 [03:42<1:23:20,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.82it/s]\n",
      "Main Epoch (Outer Loop):   4%|▍         | 45/1024 [03:47<1:23:06,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.89it/s]\n",
      "Main Epoch (Outer Loop):   4%|▍         | 46/1024 [03:52<1:22:50,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.80it/s]\n",
      "Main Epoch (Outer Loop):   5%|▍         | 47/1024 [03:58<1:22:46,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.92it/s]\n",
      "Main Epoch (Outer Loop):   5%|▍         | 48/1024 [04:03<1:22:31,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.92it/s]\n",
      "Main Epoch (Outer Loop):   5%|▍         | 49/1024 [04:08<1:22:19,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.93it/s]\n",
      "Main Epoch (Outer Loop):   5%|▍         | 50/1024 [04:13<1:22:09,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.90it/s]\n",
      "Main Epoch (Outer Loop):   5%|▍         | 51/1024 [04:18<1:22:04,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.89it/s]\n",
      "Main Epoch (Outer Loop):   5%|▌         | 52/1024 [04:23<1:21:56,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.54it/s]\n",
      "Main Epoch (Outer Loop):   5%|▌         | 53/1024 [04:28<1:22:28,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.85it/s]\n",
      "Main Epoch (Outer Loop):   5%|▌         | 54/1024 [04:33<1:22:19,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.87it/s]\n",
      "Main Epoch (Outer Loop):   5%|▌         | 55/1024 [04:38<1:22:11,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.81it/s]\n",
      "Main Epoch (Outer Loop):   5%|▌         | 56/1024 [04:43<1:22:04,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.78it/s]\n",
      "Main Epoch (Outer Loop):   6%|▌         | 57/1024 [04:48<1:22:04,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.80it/s]\n",
      "Main Epoch (Outer Loop):   6%|▌         | 58/1024 [04:53<1:22:02,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.77it/s]\n",
      "Main Epoch (Outer Loop):   6%|▌         | 59/1024 [04:59<1:21:59,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.85it/s]\n",
      "Main Epoch (Outer Loop):   6%|▌         | 60/1024 [05:04<1:21:47,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.80it/s]\n",
      "Main Epoch (Outer Loop):   6%|▌         | 61/1024 [05:09<1:21:46,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.70it/s]\n",
      "Main Epoch (Outer Loop):   6%|▌         | 62/1024 [05:14<1:21:49,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.79it/s]\n",
      "Main Epoch (Outer Loop):   6%|▌         | 63/1024 [05:19<1:21:41,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:05<00:00, 12.72it/s]\n",
      "Main Epoch (Outer Loop):   6%|▋         | 64/1024 [05:24<1:21:42,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.88it/s]\n",
      "Main Epoch (Outer Loop):   6%|▋         | 65/1024 [05:29<1:21:23,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.83it/s]\n",
      "Main Epoch (Outer Loop):   6%|▋         | 66/1024 [05:34<1:21:17,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.88it/s]\n",
      "Main Epoch (Outer Loop):   7%|▋         | 67/1024 [05:39<1:21:03,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.89it/s]\n",
      "Main Epoch (Outer Loop):   7%|▋         | 68/1024 [05:44<1:20:53,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.93it/s]\n",
      "Main Epoch (Outer Loop):   7%|▋         | 69/1024 [05:49<1:20:39,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/1024 (Inner K-Epochs): 100%|██████████| 64/64 [00:04<00:00, 12.95it/s]\n",
      "Main Epoch (Outer Loop):   7%|▋         | 70/1024 [05:54<1:20:25,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/1024 (Inner K-Epochs):  39%|███▉      | 25/64 [00:01<00:03, 12.84it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# function call\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trained_policy \u001b[38;5;241m=\u001b[39m \u001b[43mgrpo_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBASE_CONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgpt_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Significantly larger batch size recommended for stability\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepsilon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta_kl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbeta_kl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mentropy_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentropy_coeff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_iterations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_envs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 51\u001b[0m, in \u001b[0;36mgrpo_train\u001b[1;34m(model_config, train_dataloader, validation_dataloader, gpt_size, epochs, learning_rate, batch_size, gamma, k_epochs, epsilon, beta_kl, max_grad_norm, entropy_coeff, log_iterations, eval_iterations, device, num_envs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k_epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(k_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Inner K-Epochs)\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# print(\"Entered GRPO Optimization loop\")\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()   \u001b[38;5;66;03m# Flush out all the accumulated gradients for the weights of the model-under-training\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     new_logits \u001b[38;5;241m=\u001b[39m \u001b[43mPolicy_New\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m)\u001b[49m[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]   \u001b[38;5;66;03m# Get logits from model and only focus on the last iterations of each sample\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     new_dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(logits\u001b[38;5;241m=\u001b[39mnew_logits)\n\u001b[0;32m     53\u001b[0m     new_log_probs \u001b[38;5;241m=\u001b[39m new_dist\u001b[38;5;241m.\u001b[39mlog_prob(old_predictions)  \u001b[38;5;66;03m# Get the log probability of choosing the same action that the old policy took using the new distribution\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\brian\\DESKTOP\\Dev\\ML_Projects\\GRPO_Finetuning_Models\\utils\\previous_chapters.py:210\u001b[0m, in \u001b[0;36mGPTModel.forward\u001b[1;34m(self, in_idx)\u001b[0m\n\u001b[0;32m    208\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_embeds \u001b[38;5;241m+\u001b[39m pos_embeds  \u001b[38;5;66;03m# Shape [batch_size, num_tokens, emb_size]\u001b[39;00m\n\u001b[0;32m    209\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_emb(x)\n\u001b[1;32m--> 210\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrf_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(x)\n\u001b[0;32m    212\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_head(x)\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\brian\\DESKTOP\\Dev\\ML_Projects\\GRPO_Finetuning_Models\\utils\\previous_chapters.py:176\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# Shortcut connection for attention block\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m--> 176\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt(x)   \u001b[38;5;66;03m# Shape [batch_size, num_tokens, emb_size]\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_resid(x)\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\brian\\DESKTOP\\Dev\\ML_Projects\\GRPO_Finetuning_Models\\utils\\previous_chapters.py:131\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    129\u001b[0m var \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mvar(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unbiased\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    130\u001b[0m norm_x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(var \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m \u001b[38;5;241m*\u001b[39m norm_x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1911\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[1;32m-> 1918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1920\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# function call\n",
    "trained_policy = grpo_train(\n",
    "        model_config=BASE_CONFIG,\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=validation_dataloader,\n",
    "        gpt_size=args[\"gpt_size\"],\n",
    "        epochs=args[\"epochs\"],\n",
    "        learning_rate=args[\"learning_rate\"],\n",
    "        batch_size=args[\"batch_size\"], # Significantly larger batch size recommended for stability\n",
    "        k_epochs=args[\"k_epochs\"],\n",
    "        epsilon=args[\"epsilon\"],\n",
    "        beta_kl=args[\"beta_kl\"],\n",
    "        entropy_coeff=args[\"entropy_coeff\"],\n",
    "        log_iterations=args[\"log_iterations\"],\n",
    "        gamma=args[\"gamma\"],\n",
    "        device=args[\"device\"],\n",
    "        num_envs=args[\"num_envs\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print(\"Setting up for Training\")\n",
    "    \n",
    "    if args.device:     # Check if the user specified to use a CPU or GPU for training\n",
    "        device = args.device\n",
    "    else:\n",
    "        if args.use_cuda:   # Check if the user wanted to use CUDA if available.\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    BASE_CONFIG = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": True         # Query-key-value bias\n",
    "    }\n",
    "\n",
    "    # Transfer to argparser setup\n",
    "    gpt_size=\"gpt2-small (124M)\"\n",
    "    dataloader_batch_size=64\n",
    "    num_workers=0\n",
    "    pin_memory=True\n",
    "    tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    print(\"Creating Datasets using train, test, and validation files.\")\n",
    "\n",
    "    prepare_datasets(data_file_path=\"./sms_spam_collection/SMSSpamCollection.tsv\", store_directory=\"./sms_spam_collection/data_splits\")\n",
    "\n",
    "    train_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/train.csv\", tokenizer=tokenizer)\n",
    "    test_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/test.csv\", tokenizer=tokenizer)\n",
    "    validation_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/validation.csv\", tokenizer=tokenizer)\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=dataloader_batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=dataloader_batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "    validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=dataloader_batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "\n",
    "    print(\"Beginning Training Script\")\n",
    "    start_time=time.time()\n",
    "\n",
    "    trained_policy = grpo_train(\n",
    "        model_config=BASE_CONFIG,\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=validation_dataloader,\n",
    "        gpt_size=gpt_size,\n",
    "        epochs=args.epochs,\n",
    "        learning_rate=args.learning_rate,\n",
    "        batch_size=args.batch_size, # Significantly larger batch size recommended for stability\n",
    "        k_epochs=args.k_epochs,\n",
    "        epsilon=args.epsilon,\n",
    "        beta_kl=args.beta_kl,\n",
    "        entropy_coeff=args.entropy_coeff,\n",
    "        log_iterations=args.log_iterations,\n",
    "        gamma=args.gamma,\n",
    "        device=device,\n",
    "        num_envs=args.num_envs\n",
    "    )\n",
    "    end_time=time.time()\n",
    "\n",
    "    elapsed_time= end_time - start_time\n",
    "    hrs = int(elapsed_time / 3600)\n",
    "    min = int((elapsed_time % 3600) / 60)\n",
    "    seconds_remaining = elapsed_time - (hrs * 3600 ) - (min * 60)\n",
    "    print(f\"FINISHED MODEL TRAINING. \\nTRAINING TOOK: {hrs} Hours, {min} Minutes, and {seconds_remaining} Seconds\")\n",
    "\n",
    "\n",
    "    print(\"\\nTesting the trained policy:\")\n",
    "\n",
    "    classification_lyr = torch.nn.Softmax(dim=-1)\n",
    "    trained_policy.eval()   # Turn off dropout layers and prevent grad tracking\n",
    "    accuracy = 0.0\n",
    "    num_correct = 0.0\n",
    "    num_of_samples = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in test_dataloader:\n",
    "            batch_labels, predictions = batch_labels.to(device), predictions.to(device) # move the training data to the target device\n",
    "            logits = trained_policy(batch_inputs)[:,-1,:]\n",
    "            classifications = classification_lyr(logits)\n",
    "            class_predictions = torch.argmax(classifications, dim=-1).flatten()\n",
    "            num_of_samples += batch_labels.size(0)\n",
    "            num_correct += sum((class_predictions == batch_labels).float()).item()\n",
    "        accuracy = num_correct/num_of_samples\n",
    "        print(f\" Entire test Dataset Accuracy: {accuracy:.4f} |  {num_correct} corrct/ {num_of_samples} samples\")\n",
    "\n",
    "\n",
    "    #---------------  !!!  ---------------\n",
    "    SAVE_LOCATION = \"./model/trained_model.pth\"   # Define the model path and name of the trained model weights\n",
    "\n",
    "    if args.save_model:     # Check if the user wants to save the trained model weights\n",
    "        if args.model_output_path:     # Check if the user specified a target save location\n",
    "            SAVE_LOCATION=args.model_output_path\n",
    "        \n",
    "        torch.save(trained_policy.parameters(), f=SAVE_LOCATION)\n",
    "        print(f\"Model weights saved in: {SAVE_LOCATION}\")\n",
    "\n",
    "    print(\"Finished Running Script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab59e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming you have a way to call this function, e.g., in a main block)\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Train and test a BlackJack PPO agent.\")\n",
    "\n",
    "    # Add arguments\n",
    "    parser.add_argument('--epochs', type=int, default=2000,\n",
    "                        help='Number of training epochs.')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0003,\n",
    "                        help='Learning rate for the optimizer.')\n",
    "    parser.add_argument('--dataloader_batch_size', type=int, default=64,\n",
    "                        help='Dataloader Batch sizes for train, test, validation data files.')\n",
    "    parser.add_argument('--batch_size', type=int, default=1024,\n",
    "                        help='Batch size for training.')\n",
    "    parser.add_argument('--gpt2_size', type=str, default=\"gpt2-small (124M)\",\n",
    "                        help='GPT2 size for model construction.')\n",
    "    parser.add_argument('--k_epochs', type=int, default=128,\n",
    "                        help='Number of policy update epochs per trajectory collection.')\n",
    "    parser.add_argument('--epsilon', type=float, default=0.2,\n",
    "                        help='Clipping parameter for PPO.')\n",
    "    parser.add_argument('--beta_kl', type=float, default=0.01,\n",
    "                        help='KL divergence coefficient (for PPO-like algorithms).')\n",
    "    parser.add_argument('--entropy_coeff', type=float, default=0.001,\n",
    "                        help='Entropy regularization coefficient.')\n",
    "    parser.add_argument('--log_iterations', type=int, default=100,\n",
    "                        help='Log training progress every N iterations.')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99,\n",
    "                        help='Discount factor for rewards.')\n",
    "    parser.add_argument('--num_envs', type=int, default=16,\n",
    "                        help='Number of parallel environments for training.')\n",
    "    parser.add_argument('--use_cuda', action='store_true',\n",
    "                        help='Use CUDA if available.')\n",
    "    parser.add_argument('--device', type=str, default='cpu',\n",
    "                        help='Explicitly set device (e.g., \"cpu, cuda:0\", \"cpu\"). Overrides --use_cuda if specified.')\n",
    "    parser.add_argument('--save_model', action='store_true',\n",
    "                        help='Save the trained model weights.')\n",
    "    parser.add_argument('--model_output_path', type=str, default='blackjack_policy_model.pth',\n",
    "                        help='Path to save the trained model weights.')\n",
    "\n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c6981",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f51a4350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "grpo_train(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad0c7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL]) # add the emb_dim, n_layers, and n_heads to the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53154f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4beb6b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 5.26kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.93MiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 8.73kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:31<00:00, 15.6MiB/s] \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 541kiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 987kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.27MiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "801fbbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRPO_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
