{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0abfdc",
   "metadata": {},
   "source": [
    "Using GRPO to finetune the chat gpt2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5088d5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773d6412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.23.5\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.5.1\n",
      "pandas version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch, tiktoken, time, os, tensorflow\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "pkgs = [\"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        # \"tensorflow\", # For OpenAI's pretrained weights\n",
    "        \"pandas\"\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c3b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.8\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44482aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.previous_chapters import generate_text_simple, text_to_token_ids, token_ids_to_text,GPTModel, create_dataloader_v1, load_weights_into_gpt\n",
    "# Relative import from the gpt_download.py contained in this folder\n",
    "from utils.gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f58fba",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7688181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(data_file_path, sep=\"\\t\", header=None, column_names=[\"Label\", \"Text\"], train_frac=0.7, validation_frac=0.15, store_directory=\"./\"):\n",
    "    df = pd.read_csv(data_file_path, sep=sep, header=header, names=column_names)\n",
    "\n",
    "    # Count the instances of \"spam\"\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # Combine ham \"subset\" with \"spam\"\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "    # Shuffle the entire DataFrame\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(len(balanced_df) * train_frac)\n",
    "    validation_end = train_end + int(len(balanced_df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_df = balanced_df[:train_end]\n",
    "    validation_df = balanced_df[train_end:validation_end]\n",
    "    test_df = balanced_df[validation_end:]\n",
    "\n",
    "    train_df.to_csv(store_directory+\"/train.csv\", index=None)\n",
    "    validation_df.to_csv(store_directory+\"/validation.csv\", index=None)\n",
    "    test_df.to_csv(store_directory+\"/test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fd3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_file)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found: {csv_file}\")\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]    # For each row in the text section of the pandas data frame tokenize the text string(sentence); creates list of token IDs for each example/item of the text data\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8799bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fff92464",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/train.csv\", tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/test.csv\", tokenizer=tokenizer)\n",
    "validation_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/validation.csv\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3007212",
   "metadata": {},
   "source": [
    "## Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca4b5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_workers=0\n",
    "pin_memory=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d5c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebf8e7",
   "metadata": {},
   "source": [
    "## Building Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "923f77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_new_policy(base_config, chosen_model=\"gpt2-small (124M)\", num_classes = 2) -> GPTModel:\n",
    "    \"\"\"Build and load in the GPT2 model. Swap out the Head layer, and freeze up to the last Transformer module for transfer learning.\"\"\"\n",
    "    model_configs = {\n",
    "        \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "        \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "        \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "        \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "\n",
    "    base_config.update(model_configs[chosen_model]) # add the emb_dim, n_layers, and n_heads to the config\n",
    "\n",
    "    model_size = chosen_model.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "    settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "    model = GPTModel(base_config)\n",
    "\n",
    "    load_weights_into_gpt(model, params)\n",
    "    \n",
    "    for param in model.parameters(): # freeze model parameters\n",
    "        param.requires_grad = False \n",
    "\n",
    "    # Unfreeze the last transformer block\n",
    "    for param in model.trf_blocks[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Unfreeze the final layer normalizing layer\n",
    "    for param in model.final_norm.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model.out_head = torch.nn.Linear(in_features=base_config[\"emb_dim\"], out_features=num_classes) # reconfigure the output layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28e2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_old_policy(base_config, chosen_model=\"gpt2-small (124M)\", num_classes = 2) -> GPTModel:\n",
    "    \"\"\"Construct just the model without loading the weights\"\"\"\n",
    "    model_configs = {\n",
    "        \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "        \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "        \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "        \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "\n",
    "    base_config.update(model_configs[chosen_model]) # add the emb_dim, n_layers, and n_heads to the config\n",
    "\n",
    "    model_size = chosen_model.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "    model = GPTModel(base_config)\n",
    "\n",
    "    model.out_head = torch.nn.Linear(in_features=base_config[\"emb_dim\"], out_features=num_classes) # reconfigure the output layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "621ea64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_old_policy(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9762b99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.build_old_policy(base_config, chosen_model='gpt2-small (124M)', num_classes=2) -> utils.previous_chapters.GPTModel>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26b4dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_inputs, batch_labels in validation_dataloader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59ff6430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 92])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b14fb101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ee497",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f5ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discounted_rewards(predictions, batch_labels) -> torch.tensor:\n",
    "    \"\"\"For this implementation, use non-discounted rewards\"\"\"\n",
    "    disc_rewards = (predictions == batch_labels).float()    # Simple comparison to evaluate rewards for each example; output a tensor of floats\n",
    "    return disc_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4d3419",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_train(model_config, train_dataloader, validation_dataloader, gpt_size=\"gpt2-small (124M)\", epochs=50, learning_rate=0.0001, batch_size=64, gamma=0.99, k_epochs=64, epsilon=0.2, beta_kl=0.01, max_grad_norm=0.5, entropy_coeff=0.01, log_iterations=10, eval_iterations=10, device=\"cpu\", num_envs=None):\n",
    "    print(f\"Training Policy on {device} with {epochs} main epochs, {k_epochs} inner epochs, {learning_rate} learning rate, batch size {batch_size}, KL beta {beta_kl}.\")\n",
    "    print(f\"Using gpt2 size:{gpt_size} , logging every {log_iterations} iterations, evaluating every {eval_iterations} iterations.\")\n",
    "\n",
    "\n",
    "    Policy_New = build_new_policy(model_config, chosen_model=gpt_size, num_classes=2).to(device)   # STEP 3 || \n",
    "    Policy_New.train()\n",
    "\n",
    "    optimizer = optim.Adam(params=Policy_New.parameters(), lr=learning_rate)\n",
    "\n",
    "    Policy_Old = build_old_policy(model_config, chosen_model=gpt_size, num_classes=2).to(device)\n",
    "    Policy_Old.eval()\n",
    "    \n",
    "    classifier_lyr = torch.nn.Softmax(dim=-1)   # For validation loop\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=f\"Main Epoch (Outer Loop)\", leave=False):     # STEP 4 || \n",
    "        # STEP 5 || Sample a batch D_b from D --> OMITTED \n",
    "        # STEP 6 || Update the old policy model PI old <- PI new\n",
    "        Policy_Old.load_state_dict(Policy_New.state_dict())\n",
    "        print(\"loaded Policy Old Weights\")\n",
    "        # --- STEP 7 Collect a Batch of Experiences Using the Old Policy---\n",
    "        # for batch_inputs, batch_labels in train_dataloader:\n",
    "        #     pass\n",
    "        batch_inputs, batch_labels = next(iter(train_dataloader))\n",
    "        \n",
    "        batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device) # move the training data to the target device\n",
    "        print(\"Transferred Data\")\n",
    "        with torch.no_grad():\n",
    "            old_logits = Policy_Old(batch_inputs)[:,-1,:]   # Get logits from model and only focus on the last iterations of each sample\n",
    "            print(old_logits)\n",
    "            old_dist = torch.distributions.Categorical(logits=old_logits)\n",
    "            old_predictions = old_dist.sample() # Tensor of shape [1] ; list of predictions\n",
    "            print(f\"old_predictions: {old_predictions}\")\n",
    "            old_log_probs = old_dist.log_prob(old_predictions)\n",
    "\n",
    "        # STEP 8 || Calculate \"Discounted\" Rewards for completed trajectories\n",
    "        discounted_rewards = calculate_discounted_rewards(old_predictions, batch_labels)    # Tensor with \"discounted\" rewards per each sample in batch\n",
    "        print(\"Calculated discounted returns\")\n",
    "        # STEP 9 || Calculate the Advantage for each Trajectory using normalization\n",
    "        all_advantages_tensor = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "\n",
    "        Policy_New.train()  # Prepare NN for updates\n",
    "\n",
    "        # --- STEP 10 || GRPO Optimization ---\n",
    "        for k_epoch in tqdm(range(k_epochs), desc=f\"Epoch {epoch+1}/{epochs} (Inner K-Epochs)\", leave=True):\n",
    "            print(\"Entered GRPO Optimization loop\")\n",
    "            optimizer.zero_grad()   # Flush out all the accumulated gradients for the weights of the model-under-training\n",
    "\n",
    "            new_logits = Policy_New(batch_inputs)[:,-1,:]   # Get logits from model and only focus on the last iterations of each sample\n",
    "            new_dist = torch.distributions.Categorical(logits=new_logits)\n",
    "            new_log_probs = new_dist.log_prob(old_predictions)  # Get the log probability of choosing the same action that the old policy took using the new distribution\n",
    "            entropy = new_dist.entropy().mean() # Calculate entropy for regularization\n",
    "\n",
    "            R1_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "            unclipped_surrogate = R1_ratio * all_advantages_tensor\n",
    "            clipped_surrogate = torch.clamp(input=R1_ratio, min=1.0-epsilon, max=1.0+epsilon) * all_advantages_tensor\n",
    "\n",
    "            policy_loss = -torch.min(unclipped_surrogate, clipped_surrogate).mean()\n",
    "\n",
    "            # --- KL Divergence Calculation ---\n",
    "            # Create distributions for old policies using the trajectory states\n",
    "            # with torch.no_grad():\n",
    "            #     old_logits = Policy_Old(all_states_tensor)\n",
    "            # old_dist = torch.distributions.Categorical(logits=old_logits)\n",
    "            # INSTEAD, just reusing the calculated logits from STEP #7\n",
    "\n",
    "            # Calculate KL divergence per sample, then take the mean over the batch\n",
    "            kl_div_per_sample = torch.distributions.kl.kl_divergence(p=new_dist, q=old_dist)\n",
    "            kl_loss = kl_div_per_sample.mean() # Mean over the batch\n",
    "\n",
    "            # Total Loss for GRPO\n",
    "            total_loss = policy_loss + beta_kl * kl_loss - entropy_coeff * entropy\n",
    "\n",
    "            # STEP 11 || Policy Updates\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(Policy_New.parameters(), max_grad_norm)\n",
    "            optimizer.step()    # Update policy parameters using gradient ascent\n",
    "\n",
    "            # --- 4. Logging and Evaluation ---\n",
    "        if (epoch + 1) % log_iterations == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}, Ratio: {R1_ratio.mean().item():.5f}, Entropy Term: {entropy:.5f}\")\n",
    "\n",
    "        if (epoch + 1) % eval_iterations == 0:\n",
    "            Policy_New.eval()   # Turn off dropout layers and prevent grad tracking\n",
    "            accuracy = 0.0\n",
    "            num_correct = 0.0\n",
    "            num_of_samples = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch_inputs, batch_labels in validation_dataloader:\n",
    "                    batch_labels, predictions = batch_labels.to(device), predictions.to(device) # move the training data to the target device\n",
    "                    logits = Policy_New(batch_inputs)[:,-1,:]\n",
    "                    classifications = classifier_lyr(logits)\n",
    "                    class_predictions = torch.argmax(classifications, dim=-1).flatten()\n",
    "                    num_of_samples += batch_labels.size(0)\n",
    "                    num_correct += sum((class_predictions == batch_labels).float()).item()\n",
    "                accuracy = num_correct/num_of_samples\n",
    "                print(f\"Epoch {epoch+1} | Entire Validation Dataset Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "\n",
    "    Policy_New.eval()   # Change to eval mode for evaluation after training is complete\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return Policy_New # Return the trained policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e37bf",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d74a4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdfc9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretend that the argument parser will pass these arguments to the main function\n",
    "args = {\n",
    "    \"epochs\":10,\n",
    "    \"learning_rate\":0.0003,\n",
    "    \"dataloader_batch_size\":64,\n",
    "    \"dataloader_pin_memory\": True,\n",
    "    \"dataloader_num_workers\": 0,\n",
    "    \"batch_size\":1024, # Significantly larger batch size recommended for stability\n",
    "    \"gpt_size\":'gpt2-small (124M)',\n",
    "    \"k_epochs\":2,\n",
    "    \"epsilon\":0.2,\n",
    "    \"beta_kl\":0.01,\n",
    "    \"entropy_coeff\":0.001,\n",
    "    \"log_iterations\":5,\n",
    "    \"gamma\":None,   # Discounted Rewards\n",
    "    \"device\":'cuda',\n",
    "    \"num_envs\":None,\n",
    "    \"save_model\":True,\n",
    "    \"model_output_path\":'models/first.pt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f07896fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_datasets(data_file_path=\"./sms_spam_collection/SMSSpamCollection.tsv\", store_directory=\"./sms_spam_collection/data_splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2ac70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/train.csv\", tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/test.csv\", tokenizer=tokenizer)\n",
    "validation_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/validation.csv\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2867b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=args[\"batch_size\"], num_workers=args[\"dataloader_num_workers\"], pin_memory=args[\"dataloader_pin_memory\"], drop_last=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=args[\"batch_size\"], num_workers=args[\"dataloader_num_workers\"], pin_memory=args[\"dataloader_pin_memory\"], drop_last=True)\n",
    "validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=args[\"batch_size\"], num_workers=args[\"dataloader_num_workers\"], pin_memory=args[\"dataloader_pin_memory\"], drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b65f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Epoch (Outer Loop):   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Policy Old Weights\n",
      "Transferred Data\n",
      "tensor([[-3.8413,  5.7205],\n",
      "        [-3.7407,  5.6240],\n",
      "        [-3.7503,  5.6361],\n",
      "        ...,\n",
      "        [-3.7944,  5.7532],\n",
      "        [-3.6782,  5.6643],\n",
      "        [-3.6839,  5.6124]], device='cuda:0')\n",
      "old_predictions: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0')\n",
      "Calculated discounted returns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered GRPO Optimization loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered GRPO Optimization loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Inner K-Epochs): 100%|██████████| 2/2 [00:38<00:00, 19.21s/it]\n",
      "Main Epoch (Outer Loop):  10%|█         | 1/10 [00:51<07:46, 51.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Policy Old Weights\n",
      "Transferred Data\n",
      "tensor([[-4.4633,  7.0328],\n",
      "        [-4.3885,  6.9651],\n",
      "        [-4.3911,  6.9824],\n",
      "        ...,\n",
      "        [-4.4387,  7.0710],\n",
      "        [-4.3349,  6.9577],\n",
      "        [-4.3283,  6.9639]], device='cuda:0')\n",
      "old_predictions: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0')\n",
      "Calculated discounted returns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered GRPO Optimization loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered GRPO Optimization loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Inner K-Epochs): 100%|██████████| 2/2 [00:55<00:00, 27.81s/it]\n",
      "Main Epoch (Outer Loop):  20%|██        | 2/10 [02:26<10:16, 77.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Policy Old Weights\n",
      "Transferred Data\n",
      "tensor([[-5.2405,  7.8761],\n",
      "        [-5.1815,  7.8221],\n",
      "        [-5.1804,  7.8407],\n",
      "        ...,\n",
      "        [-5.2304,  7.9218],\n",
      "        [-5.1291,  7.8077],\n",
      "        [-5.1223,  7.8254]], device='cuda:0')\n",
      "old_predictions: tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0')\n",
      "Calculated discounted returns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered GRPO Optimization loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Inner K-Epochs):   0%|          | 0/2 [00:25<?, ?it/s]\n",
      "                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# function call\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trained_policy \u001b[38;5;241m=\u001b[39m \u001b[43mgrpo_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBASE_CONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgpt_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Significantly larger batch size recommended for stability\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepsilon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta_kl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbeta_kl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mentropy_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentropy_coeff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_iterations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_envs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 47\u001b[0m, in \u001b[0;36mgrpo_train\u001b[1;34m(model_config, train_dataloader, validation_dataloader, gpt_size, epochs, learning_rate, batch_size, gamma, k_epochs, epsilon, beta_kl, max_grad_norm, entropy_coeff, log_iterations, eval_iterations, device, num_envs)\u001b[0m\n\u001b[0;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()   \u001b[38;5;66;03m# Flush out all the accumulated gradients for the weights of the model-under-training\u001b[39;00m\n\u001b[0;32m     46\u001b[0m new_logits \u001b[38;5;241m=\u001b[39m Policy_New(batch_inputs)[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]   \u001b[38;5;66;03m# Get logits from model and only focus on the last iterations of each sample\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m new_dist \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m new_log_probs \u001b[38;5;241m=\u001b[39m new_dist\u001b[38;5;241m.\u001b[39mlog_prob(old_predictions)  \u001b[38;5;66;03m# Get the log probability of choosing the same action that the old policy took using the new distribution\u001b[39;00m\n\u001b[0;32m     49\u001b[0m entropy \u001b[38;5;241m=\u001b[39m new_dist\u001b[38;5;241m.\u001b[39mentropy()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;66;03m# Calculate entropy for regularization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\distributions\\categorical.py:72\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     71\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brian\\miniconda3\\envs\\GRPO_env\\lib\\site-packages\\torch\\distributions\\distribution.py:70\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     68\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[0;32m     69\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m---> 70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# function call\n",
    "trained_policy = grpo_train(\n",
    "        model_config=BASE_CONFIG,\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=validation_dataloader,\n",
    "        gpt_size=args[\"gpt_size\"],\n",
    "        epochs=args[\"epochs\"],\n",
    "        learning_rate=args[\"learning_rate\"],\n",
    "        batch_size=args[\"batch_size\"], # Significantly larger batch size recommended for stability\n",
    "        k_epochs=args[\"k_epochs\"],\n",
    "        epsilon=args[\"epsilon\"],\n",
    "        beta_kl=args[\"beta_kl\"],\n",
    "        entropy_coeff=args[\"entropy_coeff\"],\n",
    "        log_iterations=args[\"log_iterations\"],\n",
    "        gamma=args[\"gamma\"],\n",
    "        device=args[\"device\"],\n",
    "        num_envs=args[\"num_envs\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print(\"Setting up for Training\")\n",
    "    \n",
    "    if args.device:     # Check if the user specified to use a CPU or GPU for training\n",
    "        device = args.device\n",
    "    else:\n",
    "        if args.use_cuda:   # Check if the user wanted to use CUDA if available.\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    BASE_CONFIG = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": True         # Query-key-value bias\n",
    "    }\n",
    "\n",
    "    # Transfer to argparser setup\n",
    "    gpt_size=\"gpt2-small (124M)\"\n",
    "    dataloader_batch_size=64\n",
    "    num_workers=0\n",
    "    pin_memory=True\n",
    "    tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    print(\"Creating Datasets using train, test, and validation files.\")\n",
    "\n",
    "    prepare_datasets(data_file_path=\"./sms_spam_collection/SMSSpamCollection.tsv\", store_directory=\"./sms_spam_collection/data_splits\")\n",
    "\n",
    "    train_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/train.csv\", tokenizer=tokenizer)\n",
    "    test_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/test.csv\", tokenizer=tokenizer)\n",
    "    validation_dataset = SpamDataset(csv_file=\"./sms_spam_collection/data_splits/validation.csv\", tokenizer=tokenizer)\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=dataloader_batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=dataloader_batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "    validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=dataloader_batch_size, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "\n",
    "    print(\"Beginning Training Script\")\n",
    "    start_time=time.time()\n",
    "\n",
    "    trained_policy = grpo_train(\n",
    "        model_config=BASE_CONFIG,\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=validation_dataloader,\n",
    "        gpt_size=gpt_size,\n",
    "        epochs=args.epochs,\n",
    "        learning_rate=args.learning_rate,\n",
    "        batch_size=args.batch_size, # Significantly larger batch size recommended for stability\n",
    "        k_epochs=args.k_epochs,\n",
    "        epsilon=args.epsilon,\n",
    "        beta_kl=args.beta_kl,\n",
    "        entropy_coeff=args.entropy_coeff,\n",
    "        log_iterations=args.log_iterations,\n",
    "        gamma=args.gamma,\n",
    "        device=device,\n",
    "        num_envs=args.num_envs\n",
    "    )\n",
    "    end_time=time.time()\n",
    "\n",
    "    elapsed_time= end_time - start_time\n",
    "    hrs = int(elapsed_time / 3600)\n",
    "    min = int((elapsed_time % 3600) / 60)\n",
    "    seconds_remaining = elapsed_time - (hrs * 3600 ) - (min * 60)\n",
    "    print(f\"FINISHED MODEL TRAINING. \\nTRAINING TOOK: {hrs} Hours, {min} Minutes, and {seconds_remaining} Seconds\")\n",
    "\n",
    "\n",
    "    print(\"\\nTesting the trained policy:\")\n",
    "\n",
    "    classification_lyr = torch.nn.Softmax(dim=-1)\n",
    "    trained_policy.eval()   # Turn off dropout layers and prevent grad tracking\n",
    "    accuracy = 0.0\n",
    "    num_correct = 0.0\n",
    "    num_of_samples = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in test_dataloader:\n",
    "            batch_labels, predictions = batch_labels.to(device), predictions.to(device) # move the training data to the target device\n",
    "            logits = trained_policy(batch_inputs)[:,-1,:]\n",
    "            classifications = classification_lyr(logits)\n",
    "            class_predictions = torch.argmax(classifications, dim=-1).flatten()\n",
    "            num_of_samples += batch_labels.size(0)\n",
    "            num_correct += sum((class_predictions == batch_labels).float()).item()\n",
    "        accuracy = num_correct/num_of_samples\n",
    "        print(f\" Entire test Dataset Accuracy: {accuracy:.4f} |  {num_correct} corrct/ {num_of_samples} samples\")\n",
    "\n",
    "\n",
    "    #---------------  !!!  ---------------\n",
    "    SAVE_LOCATION = \"./model/trained_model.pth\"   # Define the model path and name of the trained model weights\n",
    "\n",
    "    if args.save_model:     # Check if the user wants to save the trained model weights\n",
    "        if args.model_output_path:     # Check if the user specified a target save location\n",
    "            SAVE_LOCATION=args.model_output_path\n",
    "        \n",
    "        torch.save(trained_policy.parameters(), f=SAVE_LOCATION)\n",
    "        print(f\"Model weights saved in: {SAVE_LOCATION}\")\n",
    "\n",
    "    print(\"Finished Running Script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab59e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming you have a way to call this function, e.g., in a main block)\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Train and test a BlackJack PPO agent.\")\n",
    "\n",
    "    # Add arguments\n",
    "    parser.add_argument('--epochs', type=int, default=2000,\n",
    "                        help='Number of training epochs.')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0003,\n",
    "                        help='Learning rate for the optimizer.')\n",
    "    parser.add_argument('--dataloader_batch_size', type=int, default=64,\n",
    "                        help='Dataloader Batch sizes for train, test, validation data files.')\n",
    "    parser.add_argument('--batch_size', type=int, default=1024,\n",
    "                        help='Batch size for training.')\n",
    "    parser.add_argument('--gpt2_size', type=str, default=\"gpt2-small (124M)\",\n",
    "                        help='GPT2 size for model construction.')\n",
    "    parser.add_argument('--k_epochs', type=int, default=128,\n",
    "                        help='Number of policy update epochs per trajectory collection.')\n",
    "    parser.add_argument('--epsilon', type=float, default=0.2,\n",
    "                        help='Clipping parameter for PPO.')\n",
    "    parser.add_argument('--beta_kl', type=float, default=0.01,\n",
    "                        help='KL divergence coefficient (for PPO-like algorithms).')\n",
    "    parser.add_argument('--entropy_coeff', type=float, default=0.001,\n",
    "                        help='Entropy regularization coefficient.')\n",
    "    parser.add_argument('--log_iterations', type=int, default=100,\n",
    "                        help='Log training progress every N iterations.')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99,\n",
    "                        help='Discount factor for rewards.')\n",
    "    parser.add_argument('--num_envs', type=int, default=16,\n",
    "                        help='Number of parallel environments for training.')\n",
    "    parser.add_argument('--use_cuda', action='store_true',\n",
    "                        help='Use CUDA if available.')\n",
    "    parser.add_argument('--device', type=str, default='cpu',\n",
    "                        help='Explicitly set device (e.g., \"cpu, cuda:0\", \"cpu\"). Overrides --use_cuda if specified.')\n",
    "    parser.add_argument('--save_model', action='store_true',\n",
    "                        help='Save the trained model weights.')\n",
    "    parser.add_argument('--model_output_path', type=str, default='blackjack_policy_model.pth',\n",
    "                        help='Path to save the trained model weights.')\n",
    "\n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c6981",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f51a4350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "grpo_train(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad0c7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL]) # add the emb_dim, n_layers, and n_heads to the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53154f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4beb6b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 5.26kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.93MiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 8.73kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:31<00:00, 15.6MiB/s] \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 541kiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 987kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.27MiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "801fbbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRPO_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
